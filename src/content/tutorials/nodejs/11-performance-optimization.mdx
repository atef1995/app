---
title: "Performance Optimization: Building Lightning-Fast Node.js Applications"
description: "Master profiling techniques, memory management, caching strategies with Redis, load balancing, and clustering. Learn to identify bottlenecks and optimize Node.js applications for production-scale performance."
difficulty: 4
tags: ["performance", "profiling", "memory", "redis", "clustering", "optimization", "monitoring"]
relatedTutorials: ["10-testing-debugging", "09-real-time-applications", "07-database-integration"]
estimatedTime: "180 minutes"
prerequisites: ["Express.js and REST API development", "Understanding of asynchronous JavaScript and promises", "Database operations and query optimization", "Basic knowledge of system architecture concepts"]
objectives:
  - "Master Node.js profiling tools to identify performance bottlenecks"
  - "Implement effective memory management and detect memory leaks"
  - "Design and implement caching strategies using Redis"
  - "Configure load balancing and clustering for scalability"
  - "Optimize a real-world slow API through systematic performance improvements"
order: 11
isPremium: true
requiredPlan: "CRACKED"
---

# Performance Optimization: Building Lightning-Fast Node.js Applications

Performance isn't just about speed—it's about delivering exceptional user experiences while efficiently using system resources. In this tutorial, we'll explore how to transform slow, resource-hungry Node.js applications into lightning-fast, scalable systems that can handle real-world production loads.

**Student**: "My Node.js API works fine in development, but it crawls under load. Users are complaining about slow response times, and the server sometimes crashes. How do professional developers build applications that stay fast even with thousands of users?"

**Teacher**: "You've hit the performance wall—every developer's rite of passage! The difference between a hobby project and a production system isn't just features, it's how it performs under pressure. Today, we'll learn to think like performance engineers: measuring first, optimizing strategically, and building systems that scale gracefully. Performance optimization is part detective work, part engineering, and part art."

## The Performance Mindset: Measure, Don't Guess

Before diving into optimization techniques, let's establish the fundamental principle: **never optimize without measuring**. Assumptions about performance bottlenecks are wrong more often than right.

### Understanding Performance Metrics

**Student**: "I know my app is slow, but how do I figure out where the problems actually are?"

**Teacher**: "Great question! Performance has multiple dimensions. Response time is what users feel, but throughput is what your business cares about. Memory usage affects stability, and CPU usage impacts scalability. We need to measure all of these to get the full picture."

The key metrics every Node.js developer should track:

- **Response Time**: How long requests take (P50, P95, P99 percentiles)
- **Throughput**: Requests per second your app can handle
- **Memory Usage**: Heap size, garbage collection frequency
- **CPU Utilization**: Processing efficiency and bottlenecks
- **Event Loop Lag**: How blocked your event loop becomes

## Profiling Node.js Applications: Finding the Bottlenecks

Let's start with the detective work—profiling tools that reveal where your application spends its time.

<InteractiveCodeBlock
  title="Built-in Node.js Profiling Techniques"
  language="nodejs"
  showOutput={true}
  editable={true}
>
{`
// Performance profiling utilities and examples
const { performance, PerformanceObserver } = require('perf_hooks');
const fs = require('fs');
const crypto = require('crypto');

class PerformanceProfiler {
  constructor() {
    this.measurements = [];
    this.setupPerformanceObserver();
  }

  setupPerformanceObserver() {
    // Observe performance marks and measures
    const obs = new PerformanceObserver((list) => {
      list.getEntries().forEach((entry) => {
        console.log(\`📊 [\${entry.entryType}] \${entry.name}: \${entry.duration?.toFixed(2)}ms\`);
        this.measurements.push({
          name: entry.name,
          duration: entry.duration,
          type: entry.entryType,
          timestamp: entry.startTime
        });
      });
    });

    obs.observe({ entryTypes: ['measure', 'mark'] });
  }

  // Measure function execution time
  async measureFunction(name, fn, ...args) {
    const markStart = \`\${name}-start\`;
    const markEnd = \`\${name}-end\`;
    
    performance.mark(markStart);
    const result = await fn(...args);
    performance.mark(markEnd);
    
    performance.measure(name, markStart, markEnd);
    
    return result;
  }

  // Measure multiple operations and compare
  async benchmarkOperations(operations) {
    const results = {};
    
    for (const [name, operation] of Object.entries(operations)) {
      const times = [];
      
      // Run each operation multiple times for accuracy
      for (let i = 0; i < 5; i++) {
        const start = performance.now();
        await operation();
        const end = performance.now();
        times.push(end - start);
      }
      
      results[name] = {
        average: times.reduce((a, b) => a + b) / times.length,
        min: Math.min(...times),
        max: Math.max(...times)
      };
    }
    
    return results;
  }

  // Get current memory usage with detailed breakdown
  getMemoryUsage() {
    const usage = process.memoryUsage();
    return {
      rss: Math.round(usage.rss / 1024 / 1024) + 'MB',
      heapUsed: Math.round(usage.heapUsed / 1024 / 1024) + 'MB',
      heapTotal: Math.round(usage.heapTotal / 1024 / 1024) + 'MB',
      external: Math.round(usage.external / 1024 / 1024) + 'MB',
      heapUsagePercentage: Math.round((usage.heapUsed / usage.heapTotal) * 100) + '%'
    };
  }

  // Monitor event loop lag
  monitorEventLoop(duration = 5000) {
    const samples = [];
    const startTime = Date.now();
    
    const sampleEventLoop = () => {
      const start = process.hrtime.bigint();
      setImmediate(() => {
        const lag = Number(process.hrtime.bigint() - start) / 1_000_000; // Convert to ms
        samples.push(lag);
        
        if (Date.now() - startTime < duration) {
          sampleEventLoop();
        } else {
          const avgLag = samples.reduce((a, b) => a + b) / samples.length;
          const maxLag = Math.max(...samples);
          console.log(\`🔄 Event Loop Analysis (\${duration/1000}s):\`);
          console.log(\`   Average Lag: \${avgLag.toFixed(2)}ms\`);
          console.log(\`   Maximum Lag: \${maxLag.toFixed(2)}ms\`);
          console.log(\`   Samples: \${samples.length}\`);
          
          if (avgLag > 10) {
            console.log('⚠️  WARNING: High event loop lag detected!');
          }
        }
      });
    };
    
    sampleEventLoop();
  }
}

// Example: Profiling different approaches to the same problem
class PerformanceDemo {
  constructor() {
    this.profiler = new PerformanceProfiler();
    this.largeDataSet = Array.from({ length: 100000 }, (_, i) => ({
      id: i,
      name: \`User \${i}\`,
      email: \`user\${i}@example.com\`,
      score: Math.floor(Math.random() * 1000)
    }));
  }

  // Inefficient approach: Multiple array iterations
  async processDataInefficient(data) {
    const activeUsers = data.filter(user => user.score > 500);
    const userNames = activeUsers.map(user => user.name);
    const sortedNames = userNames.sort();
    const topTen = sortedNames.slice(0, 10);
    return topTen;
  }

  // Efficient approach: Single pass with early termination
  async processDataEfficient(data) {
    const results = [];
    
    for (const user of data) {
      if (user.score > 500) {
        results.push(user.name);
        if (results.length >= 10) break; // Early termination
      }
    }
    
    return results.sort();
  }

  // Demonstrating synchronous vs asynchronous operations
  syncHeavyOperation() {
    let result = 0;
    for (let i = 0; i < 1000000; i++) {
      result += Math.sqrt(i);
    }
    return result;
  }

  async asyncHeavyOperation() {
    return new Promise((resolve) => {
      setImmediate(() => {
        let result = 0;
        for (let i = 0; i < 1000000; i++) {
          result += Math.sqrt(i);
        }
        resolve(result);
      });
    });
  }

  // File I/O performance comparison
  async fileOperations() {
    const data = 'x'.repeat(1000000); // 1MB of data
    const filename = '/tmp/test-file.txt';

    const operations = {
      'Synchronous Write': () => {
        fs.writeFileSync(filename, data);
      },
      
      'Asynchronous Write': () => {
        return new Promise((resolve, reject) => {
          fs.writeFile(filename, data, (err) => {
            if (err) reject(err);
            else resolve();
          });
        });
      },
      
      'Stream Write': () => {
        return new Promise((resolve, reject) => {
          const stream = fs.createWriteStream(filename);
          stream.write(data);
          stream.end();
          stream.on('finish', resolve);
          stream.on('error', reject);
        });
      }
    };

    return await this.profiler.benchmarkOperations(operations);
  }

  async runPerformanceAnalysis() {
    console.log('🧪 PERFORMANCE ANALYSIS DEMONSTRATION');
    console.log('====================================');
    
    console.log('\\n📊 Initial Memory Usage:');
    console.log(this.profiler.getMemoryUsage());
    
    console.log('\\n🔄 Event Loop Monitoring (starting...)');
    this.profiler.monitorEventLoop(3000);
    
    console.log('\\n🆚 Data Processing Comparison:');
    
    const inefficientResult = await this.profiler.measureFunction(
      'Inefficient Data Processing',
      this.processDataInefficient.bind(this),
      this.largeDataSet
    );
    
    const efficientResult = await this.profiler.measureFunction(
      'Efficient Data Processing', 
      this.processDataEfficient.bind(this),
      this.largeDataSet
    );
    
    console.log('\\n💾 Memory Usage After Processing:');
    console.log(this.profiler.getMemoryUsage());
    
    console.log('\\n⚡ Sync vs Async Operations:');
    
    performance.mark('sync-start');
    this.syncHeavyOperation();
    performance.mark('sync-end');
    performance.measure('Synchronous Heavy Operation', 'sync-start', 'sync-end');
    
    await this.profiler.measureFunction(
      'Asynchronous Heavy Operation',
      this.asyncHeavyOperation.bind(this)
    );
    
    console.log('\\n💿 File I/O Performance:');
    const fileResults = await this.fileOperations();
    
    console.log('File Operation Results:');
    Object.entries(fileResults).forEach(([name, stats]) => {
      console.log(\`  \${name}: \${stats.average.toFixed(2)}ms (avg)\`);
    });
    
    console.log('\\n📈 Performance Summary:');
    console.log('========================');
    console.log('✅ Efficient data processing is significantly faster');
    console.log('✅ Async operations don\\'t block the event loop');
    console.log('✅ Stream-based I/O is most memory efficient');
    console.log('✅ Memory usage monitoring reveals allocation patterns');
    
    // Clean up
    setTimeout(() => {
      try {
        fs.unlinkSync('/tmp/test-file.txt');
      } catch (e) {
        // File might not exist in this environment
      }
    }, 100);
  }
}

// CPU Profiling Example
class CPUProfiler {
  static startProfiling(filename = 'cpu-profile.json') {
    console.log(\`🎯 Starting CPU profiling... (use --prof flag in real Node.js)\`);
    console.log(\`   node --prof your-app.js\`);
    console.log(\`   node --prof-process isolate-*.log > \${filename}\`);
    
    // In real scenarios, you'd use:
    // require('inspector').Session.post('Profiler.enable')
    // This is a demonstration of the process
  }
  
  static analyzeProfile() {
    console.log('📊 CPU Profile Analysis:');
    console.log('========================');
    console.log('🔥 Hot Functions (most CPU time):');
    console.log('   1. processDataInefficient() - 45.2%');
    console.log('   2. Array.filter() - 23.1%');
    console.log('   3. Array.map() - 18.7%');
    console.log('   4. Array.sort() - 12.3%');
    console.log('');
    console.log('🎯 Optimization Targets:');
    console.log('   → Reduce array iterations');
    console.log('   → Use more efficient algorithms');
    console.log('   → Consider streaming for large datasets');
  }
}

// Heap Snapshot Analysis
class MemoryProfiler {
  static takeHeapSnapshot(filename = 'heap-snapshot.heapsnapshot') {
    console.log(\`🧠 Taking heap snapshot... (simulated)\`);
    console.log(\`   Real command: require('v8').writeHeapSnapshot('\${filename}')\`);
    
    // Simulate analysis results
    this.analyzeHeapSnapshot();
  }
  
  static analyzeHeapSnapshot() {
    console.log('🔍 Heap Snapshot Analysis:');
    console.log('==========================');
    console.log('📊 Memory Distribution:');
    console.log('   Strings: 45% (potential optimization target)');
    console.log('   Arrays: 35% (check for unnecessary copies)'); 
    console.log('   Objects: 15% (normal)');
    console.log('   Closures: 5% (check for memory leaks)');
    console.log('');
    console.log('⚠️  Potential Issues:');
    console.log('   → Large number of duplicate strings');
    console.log('   → Growing array that might not be cleaned up');
    console.log('   → Consider using WeakMap for temporary references');
  }
}

// Run the comprehensive performance analysis
const demo = new PerformanceDemo();
demo.runPerformanceAnalysis();

console.log('\\n🛠️  PROFILING TOOLS REFERENCE:');
console.log('==============================');
console.log('Command Line Tools:');
console.log('  node --prof app.js              # CPU profiling');
console.log('  node --inspect app.js           # Chrome DevTools debugging');
console.log('  node --trace-warnings app.js    # Warning traces');
console.log('  node --max-old-space-size=4096   # Increase heap size');
console.log('');
console.log('Third-party Tools:');
console.log('  clinic.js doctor                 # Overall performance');
console.log('  clinic.js flame                  # CPU flame graphs'); 
console.log('  clinic.js bubbleprof            # Async operation analysis');
console.log('  0x                              # CPU profiling');
console.log('  autocannon                      # Load testing');

setTimeout(() => {
  console.log('\\n🔧 PROFILING BEST PRACTICES:');
  console.log('============================');
  console.log('✅ Profile in production-like environments');
  console.log('✅ Use representative workloads for testing');
  console.log('✅ Focus on the biggest bottlenecks first (80/20 rule)');
  console.log('✅ Measure before and after optimizations');
  console.log('✅ Profile regularly, not just when problems occur');
  console.log('✅ Consider both CPU and memory profiles together');
  
  // Demonstrate advanced profiling
  CPUProfiler.startProfiling();
  CPUProfiler.analyzeProfile();
  
  console.log('');
  MemoryProfiler.takeHeapSnapshot();
}, 4000);
`}
</InteractiveCodeBlock>

**Teacher**: "Profiling is like having X-ray vision for your code. Notice how we're not guessing where the problems are—we're measuring and finding the actual bottlenecks. The performance observer API gives us precise timing data, while memory monitoring shows us exactly how resources are being used."

## Memory Management and Leak Detection

Memory leaks are silent killers in Node.js applications. Let's learn how to detect, diagnose, and prevent them.

<InteractiveCodeBlock
  title="Memory Management and Leak Detection"
  language="nodejs"
  showOutput={true}
  editable={true}
>
{`
// Comprehensive memory management and leak detection
const EventEmitter = require('events');

class MemoryManager {
  constructor() {
    this.memorySnapshots = [];
    this.leakDetectors = [];
    this.startMonitoring();
  }

  // Take periodic memory snapshots
  startMonitoring(intervalMs = 5000) {
    setInterval(() => {
      this.takeSnapshot();
      this.detectPotentialLeaks();
    }, intervalMs);
  }

  takeSnapshot() {
    const usage = process.memoryUsage();
    const snapshot = {
      timestamp: Date.now(),
      rss: usage.rss,
      heapUsed: usage.heapUsed,
      heapTotal: usage.heapTotal,
      external: usage.external,
      gc: global.gc ? this.getGCStats() : null
    };
    
    this.memorySnapshots.push(snapshot);
    
    // Keep only last 20 snapshots
    if (this.memorySnapshots.length > 20) {
      this.memorySnapshots.shift();
    }
    
    return snapshot;
  }

  getGCStats() {
    // Simulated GC stats (real implementation would use perf_hooks)
    return {
      majorGC: Math.floor(Math.random() * 10),
      minorGC: Math.floor(Math.random() * 50),
      totalGCTime: Math.floor(Math.random() * 100)
    };
  }

  // Detect potential memory leaks
  detectPotentialLeaks() {
    if (this.memorySnapshots.length < 5) return;

    const recent = this.memorySnapshots.slice(-5);
    const growth = recent.map((snap, i) => 
      i === 0 ? 0 : snap.heapUsed - recent[i - 1].heapUsed
    );

    const avgGrowth = growth.slice(1).reduce((a, b) => a + b) / (growth.length - 1);
    const totalGrowth = recent[recent.length - 1].heapUsed - recent[0].heapUsed;

    if (avgGrowth > 1024 * 1024) { // 1MB average growth
      console.warn('⚠️  MEMORY LEAK WARNING:');
      console.warn(\`   Average growth: \${Math.round(avgGrowth / 1024 / 1024)}MB per snapshot\`);
      console.warn(\`   Total growth: \${Math.round(totalGrowth / 1024 / 1024)}MB\`);
      this.analyzeLeakSources();
    }
  }

  analyzeLeakSources() {
    console.log('🔍 Analyzing potential leak sources...');
    console.log('📊 Common leak patterns to check:');
    console.log('   → Event listeners not removed');
    console.log('   → Timers not cleared'); 
    console.log('   → Closures holding references');
    console.log('   → Large objects in global scope');
    console.log('   → Database connections not closed');
  }

  // Utility to format memory size
  formatBytes(bytes) {
    if (bytes < 1024) return bytes + 'B';
    if (bytes < 1024 * 1024) return (bytes / 1024).toFixed(1) + 'KB';
    return (bytes / 1024 / 1024).toFixed(1) + 'MB';
  }

  getMemoryReport() {
    const current = this.takeSnapshot();
    const report = {
      current: {
        rss: this.formatBytes(current.rss),
        heapUsed: this.formatBytes(current.heapUsed),
        heapTotal: this.formatBytes(current.heapTotal),
        external: this.formatBytes(current.external)
      }
    };

    if (this.memorySnapshots.length > 1) {
      const previous = this.memorySnapshots[this.memorySnapshots.length - 2];
      report.growth = {
        rss: this.formatBytes(current.rss - previous.rss),
        heapUsed: this.formatBytes(current.heapUsed - previous.heapUsed)
      };
    }

    return report;
  }
}

// Common memory leak scenarios and how to avoid them
class MemoryLeakExamples {
  constructor() {
    this.leakyObjects = [];
    this.eventEmitter = new EventEmitter();
    this.timers = [];
    this.connections = new Map();
  }

  // Example 1: Event listener leak
  demonstrateEventListenerLeak() {
    console.log('\\n❌ BAD: Event Listener Leak');
    console.log('===========================');
    
    // This creates a new listener every time without removing the old one
    const leakyFunction = () => {
      const handler = (data) => {
        console.log('Processing:', data);
      };
      
      // Adding listener without removing previous ones
      this.eventEmitter.on('data', handler);
      
      // Memory leak: handler function and closure are never cleaned up
    };
    
    // Simulate multiple calls that would leak memory
    for (let i = 0; i < 5; i++) {
      leakyFunction();
    }
    
    console.log(\`📊 Event listeners count: \${this.eventEmitter.listenerCount('data')}\`);
    console.log('🔧 FIX: Always remove listeners or use once()');
    
    // Clean up for demonstration
    this.eventEmitter.removeAllListeners('data');
  }

  demonstrateCorrectEventHandling() {
    console.log('\\n✅ GOOD: Proper Event Handling');
    console.log('==============================');
    
    const correctFunction = () => {
      const handler = (data) => {
        console.log('Processing:', data);
        // Remove listener after use if it's one-time
        this.eventEmitter.removeListener('data', handler);
      };
      
      // Or use once() for one-time listeners
      this.eventEmitter.once('data', (data) => {
        console.log('One-time processing:', data);
      });
    };
    
    correctFunction();
    console.log('✅ Listeners properly managed');
  }

  // Example 2: Timer leak
  demonstrateTimerLeak() {
    console.log('\\n❌ BAD: Timer Leak');
    console.log('==================');
    
    const leakyTimer = () => {
      const timerId = setInterval(() => {
        console.log('Periodic task running...');
      }, 1000);
      
      // Timer never cleared - memory leak!
      this.timers.push(timerId);
    };
    
    // Don't actually run this for demo
    console.log('🔧 FIX: Always clear timers when done');
    
    // Proper cleanup
    this.clearAllTimers();
  }

  clearAllTimers() {
    console.log('✅ Clearing all timers...');
    this.timers.forEach(timerId => {
      clearInterval(timerId);
    });
    this.timers = [];
  }

  // Example 3: Closure leak
  demonstrateClosureLeak() {
    console.log('\\n❌ BAD: Closure Memory Leak');
    console.log('============================');
    
    const createLeakyClosure = () => {
      const largeData = new Array(100000).fill('data'); // Large array
      
      // This closure holds reference to largeData even if it doesn't use it
      const processor = () => {
        return 'processed'; // largeData still in memory due to closure
      };
      
      return processor;
    };
    
    // Each call creates a new closure with large data
    const processors = [];
    for (let i = 0; i < 5; i++) {
      processors.push(createLeakyClosure());
    }
    
    console.log('🔧 FIX: Nullify large variables in closures');
    
    // Better approach
    const createEfficientClosure = () => {
      let largeData = new Array(100000).fill('data');
      
      const processor = () => {
        return 'processed';
      };
      
      // Clear the reference
      largeData = null;
      
      return processor;
    };
    
    console.log('✅ Efficient closure created');
  }

  // Example 4: WeakMap vs Map for temporary references
  demonstrateWeakMapUsage() {
    console.log('\\n🆚 Map vs WeakMap for Temporary References');
    console.log('==========================================');
    
    // Regular Map keeps references
    const regularMap = new Map();
    const weakMap = new WeakMap();
    
    const createObjects = () => {
      for (let i = 0; i < 1000; i++) {
        const obj = { id: i, data: 'temporary data' };
        regularMap.set(obj, \`metadata-\${i}\`);
        weakMap.set(obj, \`metadata-\${i}\`);
      }
    };
    
    createObjects();
    
    console.log(\`📊 Regular Map size: \${regularMap.size}\`);
    console.log(\`📊 WeakMap allows garbage collection of unused keys\`);
    
    // Clear regular map to prevent actual leak in demo
    regularMap.clear();
    
    console.log('💡 TIP: Use WeakMap when you don\\'t control object lifecycle');
  }

  // Memory optimization techniques
  demonstrateOptimizations() {
    console.log('\\n⚡ MEMORY OPTIMIZATION TECHNIQUES');
    console.log('=================================');
    
    console.log('1️⃣ Object Pooling:');
    console.log('   → Reuse objects instead of creating new ones');
    console.log('   → Especially useful for frequently created objects');
    
    console.log('\\n2️⃣ Streaming for Large Data:');
    console.log('   → Process data in chunks instead of loading all at once');
    console.log('   → Use Node.js streams for large file operations');
    
    console.log('\\n3️⃣ Efficient Data Structures:');
    console.log('   → Use Set instead of Array for unique values');
    console.log('   → Use Map instead of Object for dynamic keys');
    console.log('   → Use Buffer for binary data instead of strings');
    
    console.log('\\n4️⃣ Memory-conscious Programming:');
    console.log('   → Delete unused object properties');
    console.log('   → Set large variables to null when done');
    console.log('   → Avoid creating unnecessary intermediate objects');
  }
}

// Garbage Collection monitoring
class GCMonitor {
  static startMonitoring() {
    console.log('\\n♻️  GARBAGE COLLECTION MONITORING');
    console.log('==================================');
    
    if (global.gc) {
      console.log('✅ Manual GC available (--expose-gc flag used)');
      
      // Monitor GC performance
      const { PerformanceObserver } = require('perf_hooks');
      const obs = new PerformanceObserver((list) => {
        list.getEntries().forEach((entry) => {
          if (entry.entryType === 'gc') {
            console.log(\`♻️  GC [\${entry.kind}]: \${entry.duration.toFixed(2)}ms\`);
          }
        });
      });
      
      // This would work in real Node.js with proper setup
      // obs.observe({ entryTypes: ['gc'] });
      
      // Force a GC cycle for demonstration
      console.log('🔄 Running manual garbage collection...');
      global.gc();
      console.log('✅ GC cycle completed');
      
    } else {
      console.log('⚠️  Manual GC not available. Run with --expose-gc flag:');
      console.log('   node --expose-gc --max-old-space-size=4096 app.js');
    }
    
    console.log('\\n📊 GC Tuning Options:');
    console.log('  --max-old-space-size=SIZE   # Set max heap size');
    console.log('  --max-new-space-size=SIZE   # Set max new generation size');
    console.log('  --gc-interval=N             # Set GC interval');
  }
  
  static getGCRecommendations() {
    console.log('\\n🎯 GC OPTIMIZATION RECOMMENDATIONS:');
    console.log('===================================');
    console.log('✅ Monitor heap growth patterns regularly');
    console.log('✅ Tune heap size based on application needs');
    console.log('✅ Use heap snapshots to identify memory hotspots');
    console.log('✅ Consider generational GC characteristics');
    console.log('✅ Reduce object allocation in hot code paths');
    console.log('✅ Use object pooling for frequently created objects');
  }
}

// Run comprehensive memory management demonstration
console.log('🧠 MEMORY MANAGEMENT COMPREHENSIVE DEMO');
console.log('=======================================');

const memoryManager = new MemoryManager();
const leakExamples = new MemoryLeakExamples();

// Demonstrate memory report
console.log('\\n📊 Initial Memory Report:');
console.log(memoryManager.getMemoryReport());

// Show leak examples
leakExamples.demonstrateEventListenerLeak();
leakExamples.demonstrateCorrectEventHandling();
leakExamples.demonstrateTimerLeak();
leakExamples.demonstrateClosureLeak();
leakExamples.demonstrateWeakMapUsage();
leakExamples.demonstrateOptimizations();

// GC monitoring
GCMonitor.startMonitoring();
GCMonitor.getGCRecommendations();

// Final memory report
setTimeout(() => {
  console.log('\\n📊 Final Memory Report:');
  console.log(memoryManager.getMemoryReport());
  
  console.log('\\n🛡️  MEMORY LEAK PREVENTION CHECKLIST:');
  console.log('=====================================');
  console.log('✅ Remove event listeners when components unmount');
  console.log('✅ Clear timers and intervals when done');
  console.log('✅ Close database connections and file handles');
  console.log('✅ Set large objects to null when finished');
  console.log('✅ Use WeakMap/WeakSet for temporary references');
  console.log('✅ Monitor memory growth in production');
  console.log('✅ Use heap snapshots to debug memory issues');
  console.log('✅ Consider using --max-old-space-size in production');
}, 2000);
`}
</InteractiveCodeBlock>

**Student**: "I never realized how many ways memory can leak in Node.js! The event listener example really hit home—I've definitely done that before."

**Teacher**: "Memory leaks are sneaky because they don't cause immediate failures. Your app works fine initially, but after running for hours or days, it starts consuming more and more memory until it crashes. The key is building awareness of these patterns and implementing monitoring to catch them early."

## Caching Strategies: Speed Through Smart Storage

Caching is often the highest-impact performance optimization. Let's explore different caching patterns and implement them with Redis.

<InteractiveCodeBlock
  title="Comprehensive Caching Strategies with Redis"
  language="nodejs"
  showOutput={true}
  editable={true}
>
{`
// Comprehensive caching implementation with multiple strategies
class CacheManager {
  constructor() {
    // In-memory cache for fastest access
    this.memoryCache = new Map();
    this.memoryCacheStats = { hits: 0, misses: 0 };
    
    // Redis client simulation
    this.redis = new RedisSimulator();
    this.redisStats = { hits: 0, misses: 0 };
    
    this.setupCacheCleanup();
  }

  // Multi-level caching: Memory -> Redis -> Database
  async get(key, fetchFunction, options = {}) {
    const { ttl = 3600, useMemoryCache = true, useRedisCache = true } = options;
    
    console.log(\`🔍 Cache lookup for key: \${key}\`);
    
    // Level 1: Memory cache (fastest)
    if (useMemoryCache && this.memoryCache.has(key)) {
      const cached = this.memoryCache.get(key);
      if (!this.isExpired(cached)) {
        this.memoryCacheStats.hits++;
        console.log(\`⚡ Memory cache HIT for \${key}\`);
        return cached.data;
      } else {
        this.memoryCache.delete(key);
        console.log(\`🕐 Memory cache EXPIRED for \${key}\`);
      }
    }

    // Level 2: Redis cache
    if (useRedisCache) {
      const redisResult = await this.redis.get(key);
      if (redisResult) {
        this.redisStats.hits++;
        console.log(\`📦 Redis cache HIT for \${key}\`);
        
        // Store in memory cache for next time
        if (useMemoryCache) {
          this.setMemoryCache(key, redisResult, ttl);
        }
        
        return redisResult;
      } else {
        this.redisStats.misses++;
        console.log(\`❌ Redis cache MISS for \${key}\`);
      }
    }

    // Level 3: Database/Source (slowest)
    console.log(\`🗄️  Fetching from source for \${key}\`);
    this.memoryCacheStats.misses++;
    
    const data = await fetchFunction();
    
    // Store in all cache levels
    if (useRedisCache) {
      await this.redis.setex(key, ttl, data);
      console.log(\`📦 Stored in Redis: \${key}\`);
    }
    
    if (useMemoryCache) {
      this.setMemoryCache(key, data, ttl);
      console.log(\`⚡ Stored in memory: \${key}\`);
    }
    
    return data;
  }

  setMemoryCache(key, data, ttl) {
    this.memoryCache.set(key, {
      data,
      expires: Date.now() + (ttl * 1000)
    });
  }

  isExpired(cached) {
    return Date.now() > cached.expires;
  }

  // Cache-aside pattern
  async cacheAsideGet(key, fetchFunction, ttl = 3600) {
    console.log(\`\\n🔄 CACHE-ASIDE pattern for: \${key}\`);
    
    // Try cache first
    let data = await this.redis.get(key);
    
    if (data) {
      console.log(\`✅ Cache hit - returning cached data\`);
      return data;
    }
    
    // Cache miss - fetch from source
    console.log(\`❌ Cache miss - fetching from source\`);
    data = await fetchFunction();
    
    // Store in cache for next time
    await this.redis.setex(key, ttl, data);
    console.log(\`📦 Data cached for future requests\`);
    
    return data;
  }

  // Write-through pattern
  async writeThroughSet(key, data, persistFunction, ttl = 3600) {
    console.log(\`\\n✍️  WRITE-THROUGH pattern for: \${key}\`);
    
    // Write to database first
    console.log(\`🗄️  Writing to persistent storage...\`);
    await persistFunction(data);
    console.log(\`✅ Data persisted successfully\`);
    
    // Then write to cache
    await this.redis.setex(key, ttl, data);
    this.setMemoryCache(key, data, ttl);
    console.log(\`📦 Data cached after successful write\`);
  }

  // Write-behind/Write-back pattern
  async writeBehindSet(key, data, persistFunction, ttl = 3600) {
    console.log(\`\\n⚡ WRITE-BEHIND pattern for: \${key}\`);
    
    // Write to cache immediately
    await this.redis.setex(key, ttl, data);
    this.setMemoryCache(key, data, ttl);
    console.log(\`📦 Data cached immediately\`);
    
    // Schedule background write to database
    setTimeout(async () => {
      try {
        console.log(\`🗄️  Background write to database for \${key}...\`);
        await persistFunction(data);
        console.log(\`✅ Background write completed for \${key}\`);
      } catch (error) {
        console.error(\`❌ Background write failed for \${key}:\`, error.message);
        // In production, you'd handle this with retry logic
      }
    }, 100); // Simulate background processing
  }

  // Refresh-ahead pattern
  async refreshAheadGet(key, fetchFunction, ttl = 3600, refreshThreshold = 0.8) {
    console.log(\`\\n🔄 REFRESH-AHEAD pattern for: \${key}\`);
    
    const cached = await this.redis.getWithTTL(key);
    
    if (cached) {
      const timeLeft = cached.ttl;
      const refreshTime = ttl * refreshThreshold;
      
      console.log(\`📊 Cache TTL remaining: \${timeLeft}s, refresh threshold: \${refreshTime}s\`);
      
      // If cache will expire soon, refresh in background
      if (timeLeft < refreshTime) {
        console.log(\`🔄 Proactively refreshing cache in background...\`);
        
        // Refresh in background without waiting
        setTimeout(async () => {
          try {
            const freshData = await fetchFunction();
            await this.redis.setex(key, ttl, freshData);
            console.log(\`✅ Background refresh completed for \${key}\`);
          } catch (error) {
            console.error(\`❌ Background refresh failed for \${key}:\`, error.message);
          }
        }, 10);
      }
      
      return cached.data;
    }
    
    // Cache miss - fetch normally
    console.log(\`❌ Cache miss - fetching from source\`);
    const data = await fetchFunction();
    await this.redis.setex(key, ttl, data);
    return data;
  }

  // Setup cleanup for memory cache
  setupCacheCleanup() {
    setInterval(() => {
      const now = Date.now();
      let cleanedCount = 0;
      
      for (const [key, cached] of this.memoryCache.entries()) {
        if (this.isExpired(cached)) {
          this.memoryCache.delete(key);
          cleanedCount++;
        }
      }
      
      if (cleanedCount > 0) {
        console.log(\`🧹 Cleaned up \${cleanedCount} expired memory cache entries\`);
      }
    }, 5000);
  }

  // Cache warming - pre-populate cache with frequently accessed data
  async warmCache(warningPatterns) {
    console.log(\`\\n🔥 CACHE WARMING - Pre-populating frequently accessed data\`);
    
    for (const pattern of warningPatterns) {
      console.log(\`   Warming: \${pattern.key}\`);
      await this.get(pattern.key, pattern.fetchFunction, { ttl: pattern.ttl });
    }
    
    console.log(\`✅ Cache warming completed for \${warningPatterns.length} patterns\`);
  }

  // Get cache statistics
  getStats() {
    const memoryHitRate = this.memoryCacheStats.hits / 
      (this.memoryCacheStats.hits + this.memoryCacheStats.misses) * 100;
    
    const redisHitRate = this.redisStats.hits / 
      (this.redisStats.hits + this.redisStats.misses) * 100;

    return {
      memory: {
        size: this.memoryCache.size,
        hits: this.memoryCacheStats.hits,
        misses: this.memoryCacheStats.misses,
        hitRate: \`\${isNaN(memoryHitRate) ? 0 : memoryHitRate.toFixed(1)}%\`
      },
      redis: {
        hits: this.redisStats.hits,
        misses: this.redisStats.misses,
        hitRate: \`\${isNaN(redisHitRate) ? 0 : redisHitRate.toFixed(1)}%\`
      }
    };
  }
}

// Simulated Redis client
class RedisSimulator {
  constructor() {
    this.data = new Map();
  }

  async get(key) {
    const item = this.data.get(key);
    if (!item) return null;
    
    if (Date.now() > item.expires) {
      this.data.delete(key);
      return null;
    }
    
    return item.value;
  }

  async setex(key, ttl, value) {
    this.data.set(key, {
      value,
      expires: Date.now() + (ttl * 1000)
    });
  }

  async getWithTTL(key) {
    const item = this.data.get(key);
    if (!item) return null;
    
    const ttl = Math.max(0, Math.ceil((item.expires - Date.now()) / 1000));
    if (ttl === 0) {
      this.data.delete(key);
      return null;
    }
    
    return {
      data: item.value,
      ttl
    };
  }

  async del(key) {
    return this.data.delete(key);
  }
}

// Cache-aware API service example
class UserService {
  constructor(cacheManager) {
    this.cache = cacheManager;
    this.database = new DatabaseSimulator();
  }

  // Cached user lookup
  async getUser(userId) {
    const cacheKey = \`user:\${userId}\`;
    
    return await this.cache.get(cacheKey, async () => {
      console.log(\`   📡 Database query for user \${userId}\`);
      return await this.database.findUser(userId);
    }, { ttl: 1800 }); // 30 minutes
  }

  // Cached user posts with pagination
  async getUserPosts(userId, page = 1, limit = 10) {
    const cacheKey = \`user:\${userId}:posts:\${page}:\${limit}\`;
    
    return await this.cache.get(cacheKey, async () => {
      console.log(\`   📡 Database query for user \${userId} posts (page \${page})\`);
      return await this.database.getUserPosts(userId, page, limit);
    }, { ttl: 600 }); // 10 minutes
  }

  // Write-through user update
  async updateUser(userId, userData) {
    const cacheKey = \`user:\${userId}\`;
    
    await this.cache.writeThroughSet(cacheKey, userData, async (data) => {
      console.log(\`   📡 Updating user \${userId} in database\`);
      return await this.database.updateUser(userId, data);
    }, 1800);
  }

  // Cache invalidation pattern
  async invalidateUserCache(userId) {
    console.log(\`🗑️  Invalidating cache for user \${userId}\`);
    
    // Remove specific user cache
    await this.cache.redis.del(\`user:\${userId}\`);
    
    // Pattern-based invalidation for related data
    const patterns = [
      \`user:\${userId}:posts:*\`,
      \`user:\${userId}:profile:*\`
    ];
    
    console.log(\`   Invalidated patterns: \${patterns.join(', ')}\`);
  }
}

// Database simulator
class DatabaseSimulator {
  constructor() {
    this.users = new Map([
      [1, { id: 1, name: 'Alice', email: 'alice@example.com', posts: 15 }],
      [2, { id: 2, name: 'Bob', email: 'bob@example.com', posts: 8 }]
    ]);
  }

  async findUser(userId) {
    // Simulate database delay
    await new Promise(resolve => setTimeout(resolve, 200));
    return this.users.get(parseInt(userId));
  }

  async getUserPosts(userId, page, limit) {
    await new Promise(resolve => setTimeout(resolve, 300));
    
    // Simulate paginated posts
    const startIndex = (page - 1) * limit;
    const posts = [];
    
    for (let i = startIndex; i < startIndex + limit; i++) {
      posts.push({
        id: i + 1,
        userId: parseInt(userId),
        title: \`Post \${i + 1} by user \${userId}\`,
        content: \`Content for post \${i + 1}...\`
      });
    }
    
    return posts;
  }

  async updateUser(userId, userData) {
    await new Promise(resolve => setTimeout(resolve, 150));
    
    const user = this.users.get(parseInt(userId));
    if (user) {
      Object.assign(user, userData);
      this.users.set(parseInt(userId), user);
    }
    
    return user;
  }
}

// Comprehensive caching demonstration
console.log('📦 COMPREHENSIVE CACHING STRATEGIES DEMO');
console.log('=========================================');

const cacheManager = new CacheManager();
const userService = new UserService(cacheManager);

async function runCachingDemo() {
  console.log('\\n1️⃣ Multi-level Caching Demo:');
  console.log('============================');
  
  // First request - cache miss
  let user1 = await userService.getUser(1);
  console.log('First request result:', user1);
  
  // Second request - cache hit
  user1 = await userService.getUser(1);
  console.log('Second request result:', user1);

  console.log('\\n2️⃣ Cache-Aside Pattern:');
  console.log('========================');
  
  const posts = await userService.getUserPosts(1, 1, 5);
  console.log('Posts retrieved:', posts.length, 'posts');

  console.log('\\n3️⃣ Write-Through Pattern:');
  console.log('==========================');
  
  await userService.updateUser(1, { name: 'Alice Updated', email: 'alice.new@example.com' });

  console.log('\\n4️⃣ Refresh-Ahead Pattern:');
  console.log('===========================');
  
  // This would trigger background refresh if cache is about to expire
  await cacheManager.refreshAheadGet('user:1', async () => {
    return await userService.database.findUser(1);
  }, 60, 0.8);

  console.log('\\n5️⃣ Cache Warming:');
  console.log('==================');
  
  const warmingPatterns = [
    {
      key: 'popular:posts',
      fetchFunction: async () => ({ posts: ['Popular post 1', 'Popular post 2'] }),
      ttl: 3600
    },
    {
      key: 'trending:users',
      fetchFunction: async () => ({ users: ['trending1', 'trending2'] }),
      ttl: 1800
    }
  ];
  
  await cacheManager.warmCache(warmingPatterns);

  console.log('\\n6️⃣ Cache Statistics:');
  console.log('=====================');
  
  const stats = cacheManager.getStats();
  console.log('📊 Cache Performance:');
  console.log('   Memory Cache:', stats.memory);
  console.log('   Redis Cache:', stats.redis);

  console.log('\\n7️⃣ Cache Invalidation:');
  console.log('=======================');
  
  await userService.invalidateUserCache(1);
  
  // After invalidation, next request will be cache miss
  const userAfterInvalidation = await userService.getUser(1);
  console.log('User after cache invalidation:', userAfterInvalidation);
}

runCachingDemo().catch(console.error);

console.log('\\n🎯 CACHING BEST PRACTICES:');
console.log('===========================');
console.log('✅ Use multi-level caching (memory → Redis → database)');
console.log('✅ Set appropriate TTL based on data update frequency');
console.log('✅ Implement cache warming for critical data');
console.log('✅ Monitor cache hit rates and adjust strategies');
console.log('✅ Use consistent cache key naming conventions');
console.log('✅ Implement proper cache invalidation patterns');
console.log('✅ Consider cache stampede prevention for hot keys');
console.log('✅ Use write-through for critical data consistency');

console.log('\\n⚡ REDIS PERFORMANCE TIPS:');
console.log('===========================');
console.log('🔧 Pipeline multiple commands for better throughput');
console.log('🔧 Use appropriate data structures (Hash, Set, List)'); 
console.log('🔧 Monitor Redis memory usage and eviction policies');
console.log('🔧 Use Redis Cluster for horizontal scaling');
console.log('🔧 Implement connection pooling for high concurrency');
console.log('🔧 Consider Redis persistence options (RDB/AOF)');
console.log('🔧 Use Redis Sentinel for high availability');
`}
</InteractiveCodeBlock>

**Teacher**: "Caching is like having a personal assistant who remembers everything you frequently need. The multi-level approach we've shown here mimics how modern processors work—fast local memory, then slower but larger shared memory, then persistent storage. Each level has different characteristics, and using them strategically can transform your application's performance."

## Load Balancing and Clustering: Scaling Beyond Single Processes

Node.js is single-threaded, but modern servers have multiple cores. Let's learn how to scale Node.js applications horizontally.

<InteractiveCodeBlock
  title="Load Balancing and Clustering Strategies"
  language="nodejs"
  showOutput={true}
  editable={true}
>
{`
const cluster = require('cluster');
const http = require('http');
const os = require('os');

// Cluster manager for horizontal scaling
class ClusterManager {
  constructor() {
    this.numWorkers = os.cpus().length;
    this.workers = new Map();
    this.workerStats = new Map();
    this.setupMasterProcess();
  }

  setupMasterProcess() {
    if (cluster.isMaster) {
      console.log(\`🏭 CLUSTER MASTER PROCESS STARTING\`);
      console.log(\`📊 Available CPUs: \${this.numWorkers}\`);
      console.log(\`🚀 Starting \${this.numWorkers} worker processes...\`);
      
      // Fork worker processes
      for (let i = 0; i < this.numWorkers; i++) {
        this.forkWorker(i);
      }
      
      this.setupMasterEventHandlers();
      this.startHealthMonitoring();
      this.startLoadBalancerServer();
      
    } else {
      // Worker process
      this.startWorkerServer();
    }
  }

  forkWorker(workerId) {
    console.log(\`🔧 Forking worker \${workerId}...\`);
    
    const worker = cluster.fork({ WORKER_ID: workerId });
    this.workers.set(worker.process.pid, worker);
    this.workerStats.set(worker.process.pid, {
      id: workerId,
      requests: 0,
      errors: 0,
      startTime: Date.now(),
      lastActivity: Date.now()
    });
    
    console.log(\`✅ Worker \${workerId} started with PID: \${worker.process.pid}\`);
  }

  setupMasterEventHandlers() {
    // Handle worker exit
    cluster.on('exit', (worker, code, signal) => {
      const stats = this.workerStats.get(worker.process.pid);
      console.log(\`💀 Worker \${stats?.id} (PID: \${worker.process.pid}) died\`);
      console.log(\`   Exit code: \${code}, Signal: \${signal}\`);
      console.log(\`   Requests handled: \${stats?.requests || 0}\`);
      
      // Clean up
      this.workers.delete(worker.process.pid);
      this.workerStats.delete(worker.process.pid);
      
      // Respawn worker
      if (!worker.exitedAfterDisconnect) {
        console.log(\`🔄 Respawning worker...\`);
        const newWorkerId = Math.max(...Array.from(this.workerStats.values()).map(s => s.id), -1) + 1;
        this.forkWorker(newWorkerId);
      }
    });

    // Handle worker online
    cluster.on('online', (worker) => {
      console.log(\`🟢 Worker \${worker.process.pid} is online\`);
    });

    // Handle worker listening
    cluster.on('listening', (worker, address) => {
      console.log(\`🎧 Worker \${worker.process.pid} listening on \${address.address}:\${address.port}\`);
    });

    // Handle messages from workers
    Object.values(cluster.workers).forEach(worker => {
      worker.on('message', (message) => {
        if (message.type === 'request-handled') {
          const stats = this.workerStats.get(worker.process.pid);
          if (stats) {
            stats.requests++;
            stats.lastActivity = Date.now();
          }
        }
      });
    });
  }

  startHealthMonitoring() {
    setInterval(() => {
      console.log(\`\\n📊 CLUSTER HEALTH REPORT\`);
      console.log(\`========================\`);
      
      let totalRequests = 0;
      const now = Date.now();
      
      for (const [pid, stats] of this.workerStats) {
        const uptime = Math.round((now - stats.startTime) / 1000);
        const idleTime = Math.round((now - stats.lastActivity) / 1000);
        
        console.log(\`Worker \${stats.id} (PID: \${pid}):\`);
        console.log(\`  Requests: \${stats.requests}\`);
        console.log(\`  Uptime: \${uptime}s\`);
        console.log(\`  Idle: \${idleTime}s\`);
        console.log(\`  Status: \${idleTime > 30 ? '💤 Idle' : '🏃 Active'}\`);
        
        totalRequests += stats.requests;
      }
      
      console.log(\`\\nCluster Totals:\`);
      console.log(\`  Active Workers: \${this.workers.size}\`);
      console.log(\`  Total Requests: \${totalRequests}\`);
      console.log(\`  Avg per Worker: \${Math.round(totalRequests / this.workers.size)}\`);
      
    }, 10000); // Every 10 seconds
  }

  startLoadBalancerServer() {
    // Simple load balancer demonstration
    const server = http.createServer((req, res) => {
      // This is handled automatically by Node.js cluster module
      // but here's how you might implement custom load balancing
      
      res.writeHead(200, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({
        message: 'Load Balancer Response',
        timestamp: new Date().toISOString(),
        workers: this.workers.size
      }));
    });

    server.listen(3001, () => {
      console.log(\`\\n🚦 Load balancer listening on port 3001\`);
    });
  }

  startWorkerServer() {
    const workerId = process.env.WORKER_ID;
    
    console.log(\`👷 Worker \${workerId} (PID: \${process.pid}) starting server...\`);
    
    const server = http.createServer(async (req, res) => {
      const start = Date.now();
      
      try {
        // Simulate different types of work
        if (req.url === '/cpu-intensive') {
          await this.handleCPUIntensiveTask(req, res, workerId);
        } else if (req.url === '/io-intensive') {
          await this.handleIOIntensiveTask(req, res, workerId);
        } else if (req.url === '/health') {
          this.handleHealthCheck(req, res, workerId);
        } else {
          this.handleDefaultRequest(req, res, workerId);
        }
        
        // Report to master
        process.send({ type: 'request-handled', workerId, duration: Date.now() - start });
        
      } catch (error) {
        console.error(\`❌ Worker \${workerId} error:\`, error.message);
        res.writeHead(500, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ error: 'Internal Server Error', worker: workerId }));
      }
    });

    server.listen(3000, () => {
      console.log(\`✅ Worker \${workerId} server listening on port 3000\`);
    });

    // Graceful shutdown handler
    process.on('SIGTERM', () => {
      console.log(\`🛑 Worker \${workerId} received SIGTERM, shutting down gracefully...\`);
      server.close(() => {
        console.log(\`✅ Worker \${workerId} server closed\`);
        process.exit(0);
      });
    });
  }

  async handleCPUIntensiveTask(req, res, workerId) {
    console.log(\`⚡ Worker \${workerId} handling CPU-intensive task\`);
    
    // Simulate CPU-bound work
    let result = 0;
    const iterations = 1000000;
    
    for (let i = 0; i < iterations; i++) {
      result += Math.sqrt(i);
    }
    
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      message: 'CPU-intensive task completed',
      result: result,
      worker: workerId,
      pid: process.pid,
      iterations
    }));
  }

  async handleIOIntensiveTask(req, res, workerId) {
    console.log(\`💿 Worker \${workerId} handling I/O-intensive task\`);
    
    // Simulate I/O-bound work with multiple async operations
    const promises = [];
    for (let i = 0; i < 5; i++) {
      promises.push(
        new Promise(resolve => {
          setTimeout(() => resolve(\`IO operation \${i + 1} result\`), 100 + Math.random() * 200);
        })
      );
    }
    
    const results = await Promise.all(promises);
    
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      message: 'I/O-intensive task completed',
      results,
      worker: workerId,
      pid: process.pid
    }));
  }

  handleHealthCheck(req, res, workerId) {
    const usage = process.memoryUsage();
    const cpuUsage = process.cpuUsage();
    
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      status: 'healthy',
      worker: workerId,
      pid: process.pid,
      uptime: Math.round(process.uptime()),
      memory: {
        rss: Math.round(usage.rss / 1024 / 1024) + 'MB',
        heapUsed: Math.round(usage.heapUsed / 1024 / 1024) + 'MB',
        heapTotal: Math.round(usage.heapTotal / 1024 / 1024) + 'MB'
      },
      cpu: {
        user: cpuUsage.user,
        system: cpuUsage.system
      }
    }));
  }

  handleDefaultRequest(req, res, workerId) {
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      message: 'Hello from worker',
      worker: workerId,
      pid: process.pid,
      url: req.url,
      method: req.method,
      timestamp: new Date().toISOString()
    }));
  }

  // Graceful cluster shutdown
  gracefulShutdown() {
    console.log(\`\\n🛑 Initiating graceful cluster shutdown...\`);
    
    for (const worker of Object.values(cluster.workers)) {
      worker.send('shutdown');
      worker.disconnect();
      
      setTimeout(() => {
        if (!worker.isDead()) {
          console.log(\`⚠️  Force killing worker \${worker.process.pid}\`);
          worker.kill();
        }
      }, 5000);
    }
    
    setTimeout(() => {
      console.log(\`✅ Cluster shutdown complete\`);
      process.exit(0);
    }, 10000);
  }
}

// Load balancing strategies
class LoadBalancingStrategies {
  constructor() {
    this.servers = [
      { id: 1, host: 'localhost', port: 3001, weight: 1, connections: 0 },
      { id: 2, host: 'localhost', port: 3002, weight: 2, connections: 0 },
      { id: 3, host: 'localhost', port: 3003, weight: 1, connections: 0 }
    ];
    this.roundRobinIndex = 0;
  }

  // Round-robin load balancing
  roundRobin() {
    const server = this.servers[this.roundRobinIndex];
    this.roundRobinIndex = (this.roundRobinIndex + 1) % this.servers.length;
    console.log(\`🔄 Round-robin selected server \${server.id}\`);
    return server;
  }

  // Weighted round-robin
  weightedRoundRobin() {
    let totalWeight = 0;
    let cumulativeWeights = [];
    
    for (const server of this.servers) {
      totalWeight += server.weight;
      cumulativeWeights.push(totalWeight);
    }
    
    const random = Math.floor(Math.random() * totalWeight);
    const serverIndex = cumulativeWeights.findIndex(weight => random < weight);
    const server = this.servers[serverIndex];
    
    console.log(\`⚖️  Weighted round-robin selected server \${server.id} (weight: \${server.weight})\`);
    return server;
  }

  // Least connections
  leastConnections() {
    const server = this.servers.reduce((min, current) => 
      current.connections < min.connections ? current : min
    );
    
    console.log(\`🔗 Least connections selected server \${server.id} (\${server.connections} connections)\`);
    return server;
  }

  // Demonstrate load balancing strategies
  demonstrateStrategies() {
    console.log(\`\\n🚦 LOAD BALANCING STRATEGIES DEMO\`);
    console.log(\`=================================\`);
    
    console.log(\`\\nAvailable servers:\`);
    this.servers.forEach(server => {
      console.log(\`  Server \${server.id}: weight=\${server.weight}, connections=\${server.connections}\`);
    });
    
    console.log(\`\\n1️⃣ Round-Robin (5 requests):\`);
    for (let i = 0; i < 5; i++) {
      this.roundRobin();
    }
    
    console.log(\`\\n2️⃣ Weighted Round-Robin (5 requests):\`);
    for (let i = 0; i < 5; i++) {
      this.weightedRoundRobin();
    }
    
    // Simulate different connection counts
    this.servers[0].connections = 3;
    this.servers[1].connections = 1;
    this.servers[2].connections = 5;
    
    console.log(\`\\n3️⃣ Least Connections (5 requests):\`);
    console.log(\`   Current connections: [\${this.servers.map(s => s.connections).join(', ')}]\`);
    for (let i = 0; i < 5; i++) {
      const server = this.leastConnections();
      server.connections++; // Simulate connection increase
    }
  }
}

// Reverse proxy implementation
class ReverseProxy {
  constructor(backendServers) {
    this.backends = backendServers;
    this.loadBalancer = new LoadBalancingStrategies();
  }

  createProxyServer() {
    console.log(\`\\n🔀 REVERSE PROXY SERVER\`);
    console.log(\`======================\`);
    
    const server = http.createServer((req, res) => {
      const backend = this.loadBalancer.leastConnections();
      
      console.log(\`🔀 Proxying \${req.method} \${req.url} to server \${backend.id}\`);
      
      // In a real implementation, you'd forward the request to the backend
      // Here we'll simulate the response
      const proxyResponse = {
        message: 'Response from backend server',
        backend: backend.id,
        request: {
          method: req.method,
          url: req.url,
          headers: Object.keys(req.headers).length
        },
        timestamp: new Date().toISOString()
      };
      
      res.writeHead(200, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify(proxyResponse, null, 2));
    });

    return server;
  }
}

// Performance monitoring for clusters
class ClusterPerformanceMonitor {
  constructor() {
    this.metrics = {
      requests: 0,
      errors: 0,
      responseTime: [],
      workerMetrics: new Map()
    };
  }

  recordRequest(workerId, responseTime, isError = false) {
    this.metrics.requests++;
    if (isError) this.metrics.errors++;
    
    this.metrics.responseTime.push(responseTime);
    
    if (!this.metrics.workerMetrics.has(workerId)) {
      this.metrics.workerMetrics.set(workerId, {
        requests: 0,
        errors: 0,
        totalResponseTime: 0
      });
    }
    
    const workerStats = this.metrics.workerMetrics.get(workerId);
    workerStats.requests++;
    if (isError) workerStats.errors++;
    workerStats.totalResponseTime += responseTime;
  }

  getMetrics() {
    const avgResponseTime = this.metrics.responseTime.length > 0 
      ? this.metrics.responseTime.reduce((a, b) => a + b) / this.metrics.responseTime.length
      : 0;

    const p95ResponseTime = this.metrics.responseTime.length > 0
      ? this.metrics.responseTime.sort((a, b) => a - b)[Math.floor(this.metrics.responseTime.length * 0.95)]
      : 0;

    return {
      totalRequests: this.metrics.requests,
      totalErrors: this.metrics.errors,
      errorRate: ((this.metrics.errors / this.metrics.requests) * 100).toFixed(2) + '%',
      avgResponseTime: avgResponseTime.toFixed(2) + 'ms',
      p95ResponseTime: p95ResponseTime.toFixed(2) + 'ms',
      workerBreakdown: Array.from(this.metrics.workerMetrics.entries()).map(([id, stats]) => ({
        workerId: id,
        requests: stats.requests,
        avgResponseTime: (stats.totalResponseTime / stats.requests).toFixed(2) + 'ms',
        errorRate: ((stats.errors / stats.requests) * 100).toFixed(2) + '%'
      }))
    };
  }

  displayMetrics() {
    const metrics = this.getMetrics();
    
    console.log(\`\\n📈 CLUSTER PERFORMANCE METRICS\`);
    console.log(\`==============================\`);
    console.log(\`Total Requests: \${metrics.totalRequests}\`);
    console.log(\`Error Rate: \${metrics.errorRate}\`);
    console.log(\`Average Response Time: \${metrics.avgResponseTime}\`);
    console.log(\`95th Percentile Response Time: \${metrics.p95ResponseTime}\`);
    
    console.log(\`\\nWorker Performance:\`);
    metrics.workerBreakdown.forEach(worker => {
      console.log(\`  Worker \${worker.workerId}: \${worker.requests} requests, \${worker.avgResponseTime} avg, \${worker.errorRate} errors\`);
    });
  }
}

// Demonstration
console.log(\`🏭 NODE.JS CLUSTERING AND LOAD BALANCING DEMO\`);
console.log(\`==============================================\`);

// Show load balancing strategies
const strategies = new LoadBalancingStrategies();
strategies.demonstrateStrategies();

// Show performance monitoring
const monitor = new ClusterPerformanceMonitor();

// Simulate some requests
console.log(\`\\n📊 Simulating cluster requests...\`);
for (let i = 0; i < 20; i++) {
  const workerId = Math.floor(Math.random() * 4) + 1;
  const responseTime = 50 + Math.random() * 200;
  const isError = Math.random() < 0.1; // 10% error rate
  
  monitor.recordRequest(workerId, responseTime, isError);
}

monitor.displayMetrics();

console.log(\`\\n🔧 CLUSTERING BEST PRACTICES:\`);
console.log(\`=============================\`);
console.log(\`✅ Use one worker per CPU core for CPU-bound tasks\`);
console.log(\`✅ Implement graceful shutdown for zero-downtime deployments\`);
console.log(\`✅ Monitor worker health and restart failed processes\`);
console.log(\`✅ Use sticky sessions for stateful applications\`);
console.log(\`✅ Implement proper load balancing strategies\`);
console.log(\`✅ Use shared state (Redis) instead of in-memory state\`);
console.log(\`✅ Configure reverse proxy (nginx/HAProxy) for production\`);

console.log(\`\\n⚡ SCALING STRATEGIES:\`);
console.log(\`=====================\`);
console.log(\`🚀 Horizontal Scaling: Add more servers/containers\`);
console.log(\`📈 Vertical Scaling: Increase server resources\`);
console.log(\`🔄 Auto-scaling: Scale based on metrics (CPU, memory, requests)\`);
console.log(\`🌐 CDN: Use Content Delivery Networks for static assets\`);
console.log(\`🗄️  Database Scaling: Read replicas, sharding, caching\`);
console.log(\`⚖️  Load Balancing: Distribute traffic across instances\`);

// In a real application, you would start the cluster manager:
// const clusterManager = new ClusterManager();
console.log(\`\\n💡 To run actual cluster:\`);
console.log(\`   const clusterManager = new ClusterManager();\`);
console.log(\`   // This will fork worker processes based on CPU cores\`);
`}
</InteractiveCodeBlock>

**Student**: "This clustering concept is mind-blowing! I always wondered how big applications handle millions of users. But how do I know when my single-threaded app needs clustering?"

**Teacher**: "Great intuition! Single-threaded Node.js works well until you hit CPU bottlenecks—when your server's CPU usage consistently hits 100% on one core while others are idle. Clustering helps with CPU-bound tasks, but remember: if your bottleneck is I/O (database queries, API calls), you need to optimize those first. Clustering won't help with a slow database!"

## The Ultimate Challenge: Optimizing a Slow API

Let's put all our knowledge together and systematically optimize a deliberately slow API.

<InteractiveCodeBlock
  title="Interactive Slow API Optimization Challenge"
  language="nodejs"
  showOutput={true}
  editable={true}
>
{`
// A deliberately slow and inefficient API that we'll optimize step by step
const express = require('express');

class SlowAPIServer {
  constructor() {
    this.app = express();
    this.database = new SlowDatabase();
    this.cache = null; // Will be initialized in optimization
    this.setupMiddleware();
    this.setupRoutes();
    this.performanceMetrics = new PerformanceTracker();
  }

  setupMiddleware() {
    this.app.use(express.json());
    
    // Performance tracking middleware
    this.app.use((req, res, next) => {
      req.startTime = Date.now();
      res.on('finish', () => {
        const duration = Date.now() - req.startTime;
        this.performanceMetrics.recordRequest(req.path, duration, res.statusCode);
      });
      next();
    });
  }

  setupRoutes() {
    // ❌ SLOW: Inefficient user lookup
    this.app.get('/api/users/:id', async (req, res) => {
      console.log(\`🐌 [SLOW] Getting user \${req.params.id} - No caching, inefficient queries\`);
      
      // Multiple unnecessary database calls
      const user = await this.database.getUser(req.params.id);
      const userProfile = await this.database.getUserProfile(req.params.id);
      const userPreferences = await this.database.getUserPreferences(req.params.id);
      const userStats = await this.database.getUserStats(req.params.id);
      
      // Inefficient data processing
      const processedData = this.processUserDataSlow({
        ...user,
        profile: userProfile,
        preferences: userPreferences,
        stats: userStats
      });
      
      res.json(processedData);
    });

    // ❌ SLOW: N+1 query problem
    this.app.get('/api/posts', async (req, res) => {
      console.log(\`🐌 [SLOW] Getting posts - N+1 query problem\`);
      
      const posts = await this.database.getAllPosts();
      
      // N+1 problem: One query for posts, then N queries for authors
      for (const post of posts) {
        post.author = await this.database.getUser(post.authorId);
        
        // Even more queries for each post
        post.comments = await this.database.getPostComments(post.id);
        post.likes = await this.database.getPostLikes(post.id);
      }
      
      res.json(posts);
    });

    // ❌ SLOW: Inefficient search without indexing
    this.app.get('/api/search', async (req, res) => {
      console.log(\`🐌 [SLOW] Searching - No indexing, full table scan\`);
      
      const query = req.query.q;
      const results = await this.database.searchEverything(query);
      
      // Inefficient sorting and filtering in memory
      const processedResults = results
        .filter(item => item.content.toLowerCase().includes(query.toLowerCase()))
        .sort((a, b) => a.relevanceScore - b.relevanceScore)
        .slice(0, 10);
      
      res.json(processedResults);
    });

    // Performance stats endpoint
    this.app.get('/api/performance', (req, res) => {
      res.json(this.performanceMetrics.getStats());
    });
  }

  // ❌ SLOW: Inefficient data processing
  processUserDataSlow(userData) {
    // Unnecessary loops and operations
    let processedData = JSON.parse(JSON.stringify(userData)); // Deep clone (slow)
    
    // Simulate heavy computation
    for (let i = 0; i < 10000; i++) {
      processedData.computedField = Math.sqrt(i);
    }
    
    // Multiple array operations instead of single pass
    if (processedData.stats && processedData.stats.loginHistory) {
      processedData.stats.loginHistory = processedData.stats.loginHistory
        .filter(login => login.successful)
        .map(login => ({ ...login, formatted: new Date(login.timestamp).toISOString() }))
        .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
    }
    
    return processedData;
  }

  // ✅ OPTIMIZED: Fast user lookup with caching
  optimizedUserLookup() {
    this.app.get('/api/v2/users/:id', async (req, res) => {
      console.log(\`⚡ [OPTIMIZED] Getting user \${req.params.id} - With caching and efficient queries\`);
      
      const cacheKey = \`user_full_\${req.params.id}\`;
      
      // Check cache first
      let userData = this.cache ? await this.cache.get(cacheKey) : null;
      
      if (!userData) {
        console.log(\`   📡 Cache miss - fetching from database\`);
        // Single optimized query instead of multiple calls
        userData = await this.database.getUserWithAllData(req.params.id);
        
        // Cache for 15 minutes
        if (this.cache) {
          await this.cache.set(cacheKey, userData, 900);
        }
      } else {
        console.log(\`   ⚡ Cache hit - returning cached data\`);
      }
      
      // Efficient data processing
      const processedData = this.processUserDataFast(userData);
      
      res.json(processedData);
    });
  }

  // ✅ OPTIMIZED: Solve N+1 problem with batch loading
  optimizedPostsEndpoint() {
    this.app.get('/api/v2/posts', async (req, res) => {
      console.log(\`⚡ [OPTIMIZED] Getting posts - Batch loading, no N+1 problem\`);
      
      // Single query with JOINs instead of N+1 queries
      const postsWithAuthors = await this.database.getPostsWithAuthors();
      
      // Batch load additional data
      const postIds = postsWithAuthors.map(post => post.id);
      const [comments, likes] = await Promise.all([
        this.database.getCommentsForPosts(postIds),
        this.database.getLikesForPosts(postIds)
      ]);
      
      // Efficiently combine data
      const commentsMap = new Map();
      const likesMap = new Map();
      
      comments.forEach(comment => {
        if (!commentsMap.has(comment.postId)) {
          commentsMap.set(comment.postId, []);
        }
        commentsMap.get(comment.postId).push(comment);
      });
      
      likes.forEach(like => {
        if (!likesMap.has(like.postId)) {
          likesMap.set(like.postId, []);
        }
        likesMap.get(like.postId).push(like);
      });
      
      // Combine all data
      const enrichedPosts = postsWithAuthors.map(post => ({
        ...post,
        comments: commentsMap.get(post.id) || [],
        likes: likesMap.get(post.id) || []
      }));
      
      res.json(enrichedPosts);
    });
  }

  // ✅ OPTIMIZED: Fast search with caching
  optimizedSearchEndpoint() {
    this.app.get('/api/v2/search', async (req, res) => {
      console.log(\`⚡ [OPTIMIZED] Searching - With caching and database indexing\`);
      
      const query = req.query.q;
      const cacheKey = \`search_\${Buffer.from(query).toString('base64')}\`;
      
      let results = this.cache ? await this.cache.get(cacheKey) : null;
      
      if (!results) {
        console.log(\`   📡 Search cache miss - querying database\`);
        // Database does the heavy lifting with proper indexing
        results = await this.database.searchOptimized(query);
        
        // Cache search results for 5 minutes
        if (this.cache) {
          await this.cache.set(cacheKey, results, 300);
        }
      } else {
        console.log(\`   ⚡ Search cache hit\`);
      }
      
      res.json(results);
    });
  }

  // ✅ OPTIMIZED: Fast data processing
  processUserDataFast(userData) {
    // Pre-allocate result object
    const processedData = {
      id: userData.id,
      name: userData.name,
      email: userData.email,
      profile: userData.profile,
      preferences: userData.preferences,
      stats: userData.stats,
      computedField: 0 // Pre-computed or calculated efficiently
    };
    
    // Single-pass processing for login history
    if (userData.stats?.loginHistory) {
      const now = new Date();
      processedData.stats.loginHistory = userData.stats.loginHistory
        .reduce((acc, login) => {
          if (login.successful) {
            acc.push({
              ...login,
              formatted: new Date(login.timestamp).toISOString(),
              daysAgo: Math.floor((now - new Date(login.timestamp)) / (1000 * 60 * 60 * 24))
            });
          }
          return acc;
        }, [])
        .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
    }
    
    return processedData;
  }

  // Enable all optimizations
  enableOptimizations() {
    console.log(\`\\n🔧 ENABLING OPTIMIZATIONS\`);
    console.log(\`========================\`);
    
    // Initialize cache
    this.cache = new FastCache();
    console.log(\`✅ Cache system initialized\`);
    
    // Add optimized endpoints
    this.optimizedUserLookup();
    this.optimizedPostsEndpoint();
    this.optimizedSearchEndpoint();
    console.log(\`✅ Optimized endpoints added\`);
    
    // Add performance monitoring
    this.addPerformanceMiddleware();
    console.log(\`✅ Performance monitoring enhanced\`);
  }

  addPerformanceMiddleware() {
    // Add response time headers
    this.app.use((req, res, next) => {
      const start = process.hrtime.bigint();
      
      res.on('finish', () => {
        const duration = Number(process.hrtime.bigint() - start) / 1_000_000;
        res.set('X-Response-Time', \`\${duration.toFixed(2)}ms\`);
        
        if (duration > 1000) { // Log slow requests
          console.log(\`🐌 Slow request: \${req.method} \${req.path} took \${duration.toFixed(2)}ms\`);
        }
      });
      
      next();
    });
  }
}

// Simulated slow database
class SlowDatabase {
  constructor() {
    // Simulate database with inefficient access patterns
    this.users = new Map();
    this.posts = [];
    this.comments = [];
    this.likes = [];
    
    this.initializeData();
  }

  initializeData() {
    // Create sample data
    for (let i = 1; i <= 100; i++) {
      this.users.set(i, {
        id: i,
        name: \`User \${i}\`,
        email: \`user\${i}@example.com\`
      });
      
      this.posts.push({
        id: i,
        authorId: Math.floor(Math.random() * 100) + 1,
        title: \`Post \${i}\`,
        content: \`Content for post \${i} with keywords for searching\`
      });
    }
    
    // Generate comments and likes
    for (let i = 1; i <= 500; i++) {
      this.comments.push({
        id: i,
        postId: Math.floor(Math.random() * 100) + 1,
        content: \`Comment \${i}\`
      });
      
      this.likes.push({
        id: i,
        postId: Math.floor(Math.random() * 100) + 1,
        userId: Math.floor(Math.random() * 100) + 1
      });
    }
  }

  // ❌ SLOW: Multiple separate queries
  async getUser(id) {
    await this.simulateDelay(50);
    return this.users.get(parseInt(id));
  }

  async getUserProfile(id) {
    await this.simulateDelay(75);
    return { bio: \`Bio for user \${id}\`, avatar: \`avatar\${id}.jpg\` };
  }

  async getUserPreferences(id) {
    await this.simulateDelay(30);
    return { theme: 'dark', notifications: true };
  }

  async getUserStats(id) {
    await this.simulateDelay(100);
    return { 
      loginCount: Math.floor(Math.random() * 1000),
      loginHistory: Array.from({ length: 10 }, (_, i) => ({
        timestamp: new Date(Date.now() - i * 24 * 60 * 60 * 1000),
        successful: Math.random() > 0.1
      }))
    };
  }

  // ✅ OPTIMIZED: Single query with all data
  async getUserWithAllData(id) {
    await this.simulateDelay(120); // Single delay instead of multiple
    
    const user = this.users.get(parseInt(id));
    if (!user) return null;
    
    return {
      ...user,
      profile: { bio: \`Bio for user \${id}\`, avatar: \`avatar\${id}.jpg\` },
      preferences: { theme: 'dark', notifications: true },
      stats: {
        loginCount: Math.floor(Math.random() * 1000),
        loginHistory: Array.from({ length: 10 }, (_, i) => ({
          timestamp: new Date(Date.now() - i * 24 * 60 * 60 * 1000),
          successful: Math.random() > 0.1
        }))
      }
    };
  }

  // ❌ SLOW: N+1 queries
  async getAllPosts() {
    await this.simulateDelay(100);
    return [...this.posts];
  }

  async getPostComments(postId) {
    await this.simulateDelay(20);
    return this.comments.filter(comment => comment.postId === postId);
  }

  async getPostLikes(postId) {
    await this.simulateDelay(15);
    return this.likes.filter(like => like.postId === postId);
  }

  // ✅ OPTIMIZED: Single query with JOINs
  async getPostsWithAuthors() {
    await this.simulateDelay(150); // One larger delay instead of many small ones
    
    return this.posts.map(post => ({
      ...post,
      author: this.users.get(post.authorId)
    }));
  }

  async getCommentsForPosts(postIds) {
    await this.simulateDelay(50);
    return this.comments.filter(comment => postIds.includes(comment.postId));
  }

  async getLikesForPosts(postIds) {
    await this.simulateDelay(40);
    return this.likes.filter(like => postIds.includes(like.postId));
  }

  // ❌ SLOW: Full table scan
  async searchEverything(query) {
    await this.simulateDelay(300); // Very slow
    
    const results = [];
    
    // Inefficient search through all data
    for (const [id, user] of this.users) {
      if (user.name.toLowerCase().includes(query.toLowerCase())) {
        results.push({ type: 'user', ...user, relevanceScore: Math.random() });
      }
    }
    
    for (const post of this.posts) {
      if (post.content.toLowerCase().includes(query.toLowerCase())) {
        results.push({ type: 'post', ...post, relevanceScore: Math.random() });
      }
    }
    
    return results;
  }

  // ✅ OPTIMIZED: Indexed search
  async searchOptimized(query) {
    await this.simulateDelay(50); // Much faster with proper indexing
    
    // Simulate efficient database search with indexing
    const results = [];
    const queryLower = query.toLowerCase();
    
    // Pre-filtered and sorted by relevance
    this.posts
      .filter(post => post.content.toLowerCase().includes(queryLower))
      .sort((a, b) => {
        // Simulate relevance scoring
        const aScore = (a.content.match(new RegExp(queryLower, 'gi')) || []).length;
        const bScore = (b.content.match(new RegExp(queryLower, 'gi')) || []).length;
        return bScore - aScore;
      })
      .slice(0, 10)
      .forEach(post => {
        results.push({ 
          type: 'post', 
          ...post, 
          author: this.users.get(post.authorId),
          relevanceScore: (post.content.match(new RegExp(queryLower, 'gi')) || []).length
        });
      });
    
    return results;
  }

  async simulateDelay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Fast caching implementation
class FastCache {
  constructor() {
    this.cache = new Map();
  }

  async get(key) {
    const item = this.cache.get(key);
    if (!item) return null;
    
    if (Date.now() > item.expires) {
      this.cache.delete(key);
      return null;
    }
    
    return item.data;
  }

  async set(key, data, ttl) {
    this.cache.set(key, {
      data,
      expires: Date.now() + (ttl * 1000)
    });
  }
}

// Performance tracking
class PerformanceTracker {
  constructor() {
    this.requests = [];
  }

  recordRequest(path, duration, statusCode) {
    this.requests.push({
      path,
      duration,
      statusCode,
      timestamp: Date.now()
    });
    
    // Keep only last 100 requests
    if (this.requests.length > 100) {
      this.requests.shift();
    }
  }

  getStats() {
    const now = Date.now();
    const recentRequests = this.requests.filter(req => now - req.timestamp < 60000); // Last minute
    
    const avgResponseTime = recentRequests.length > 0
      ? recentRequests.reduce((sum, req) => sum + req.duration, 0) / recentRequests.length
      : 0;
    
    const errorRate = recentRequests.length > 0
      ? (recentRequests.filter(req => req.statusCode >= 400).length / recentRequests.length) * 100
      : 0;
    
    return {
      totalRequests: this.requests.length,
      recentRequests: recentRequests.length,
      avgResponseTime: Math.round(avgResponseTime),
      errorRate: Math.round(errorRate * 100) / 100,
      slowestEndpoints: this.getSlowestEndpoints()
    };
  }

  getSlowestEndpoints() {
    const endpointStats = new Map();
    
    this.requests.forEach(req => {
      if (!endpointStats.has(req.path)) {
        endpointStats.set(req.path, { totalTime: 0, count: 0 });
      }
      const stats = endpointStats.get(req.path);
      stats.totalTime += req.duration;
      stats.count++;
    });
    
    return Array.from(endpointStats.entries())
      .map(([path, stats]) => ({
        path,
        avgTime: Math.round(stats.totalTime / stats.count),
        requestCount: stats.count
      }))
      .sort((a, b) => b.avgTime - a.avgTime)
      .slice(0, 5);
  }
}

// INTERACTIVE OPTIMIZATION DEMONSTRATION
console.log(\`🚀 SLOW API OPTIMIZATION CHALLENGE\`);
console.log(\`==================================\`);

const slowAPI = new SlowAPIServer();

console.log(\`\\n📊 BASELINE PERFORMANCE (Before Optimization):\`);
console.log(\`===============================================\`);

// Simulate some slow requests
async function simulateSlowRequests() {
  console.log(\`\\n🐌 Testing SLOW endpoints:\`);
  
  const start1 = Date.now();
  console.log(\`   GET /api/users/1 - Starting...\`);
  // Simulate the slow user endpoint
  await new Promise(resolve => setTimeout(resolve, 300)); // Simulate slow response
  console.log(\`   GET /api/users/1 - Completed in \${Date.now() - start1}ms\`);
  
  const start2 = Date.now();
  console.log(\`   GET /api/posts - Starting...\`);
  await new Promise(resolve => setTimeout(resolve, 800)); // Simulate very slow N+1 response
  console.log(\`   GET /api/posts - Completed in \${Date.now() - start2}ms\`);
  
  const start3 = Date.now();
  console.log(\`   GET /api/search?q=test - Starting...\`);
  await new Promise(resolve => setTimeout(resolve, 400)); // Simulate slow search
  console.log(\`   GET /api/search?q=test - Completed in \${Date.now() - start3}ms\`);
}

async function simulateOptimizedRequests() {
  console.log(\`\\n⚡ Testing OPTIMIZED endpoints:\`);
  
  const start1 = Date.now();
  console.log(\`   GET /api/v2/users/1 - Starting...\`);
  await new Promise(resolve => setTimeout(resolve, 50)); // Much faster with caching
  console.log(\`   GET /api/v2/users/1 - Completed in \${Date.now() - start1}ms\`);
  
  const start2 = Date.now();
  console.log(\`   GET /api/v2/posts - Starting...\`);
  await new Promise(resolve => setTimeout(resolve, 200)); // Much faster without N+1
  console.log(\`   GET /api/v2/posts - Completed in \${Date.now() - start2}ms\`);
  
  const start3 = Date.now();
  console.log(\`   GET /api/v2/search?q=test - Starting...\`);
  await new Promise(resolve => setTimeout(resolve, 60)); // Much faster with indexing and caching
  console.log(\`   GET /api/v2/search?q=test - Completed in \${Date.now() - start3}ms\`);
}

async function runOptimizationDemo() {
  await simulateSlowRequests();
  
  console.log(\`\\n🔧 APPLYING OPTIMIZATIONS...\`);
  console.log(\`============================\`);
  slowAPI.enableOptimizations();
  
  console.log(\`\\n📊 OPTIMIZED PERFORMANCE (After Optimization):\`);
  console.log(\`===============================================\`);
  
  await simulateOptimizedRequests();
  
  console.log(\`\\n📈 PERFORMANCE IMPROVEMENT SUMMARY:\`);
  console.log(\`===================================\`);
  console.log(\`User Endpoint: 300ms → 50ms (83% faster)\`);
  console.log(\`Posts Endpoint: 800ms → 200ms (75% faster)\`);
  console.log(\`Search Endpoint: 400ms → 60ms (85% faster)\`);
  console.log(\`\\nOverall API Performance: ~80% improvement\`);
  
  console.log(\`\\n🎯 OPTIMIZATION TECHNIQUES APPLIED:\`);
  console.log(\`===================================\`);
  console.log(\`✅ Database Query Optimization:\`);
  console.log(\`   → Combined multiple queries into single calls\`);
  console.log(\`   → Eliminated N+1 query problems\`);
  console.log(\`   → Added proper database indexing\`);
  console.log(\`\\n✅ Caching Strategy:\`);
  console.log(\`   → Multi-level caching (memory + Redis)\`);
  console.log(\`   → Smart cache invalidation\`);
  console.log(\`   → Appropriate TTL values\`);
  console.log(\`\\n✅ Code Optimization:\`);
  console.log(\`   → Efficient data processing algorithms\`);
  console.log(\`   → Reduced memory allocations\`);
  console.log(\`   → Single-pass data transformations\`);
  console.log(\`\\n✅ Performance Monitoring:\`);
  console.log(\`   → Response time tracking\`);
  console.log(\`   → Slow query identification\`);
  console.log(\`   → Error rate monitoring\`);
}

runOptimizationDemo().catch(console.error);

console.log(\`\\n🏆 OPTIMIZATION CHECKLIST:\`);
console.log(\`==========================\`);
console.log(\`✅ Profile first - identify actual bottlenecks\`);
console.log(\`✅ Fix the biggest problems first (80/20 rule)\`);
console.log(\`✅ Measure before and after each optimization\`);
console.log(\`✅ Consider the full request lifecycle\`);
console.log(\`✅ Optimize databases queries and indexing\`);
console.log(\`✅ Implement appropriate caching strategies\`);
console.log(\`✅ Use efficient algorithms and data structures\`);
console.log(\`✅ Monitor performance continuously\`);
console.log(\`✅ Plan for scalability from the beginning\`);

console.log(\`\\n💡 PERFORMANCE OPTIMIZATION PRINCIPLES:\`);
console.log(\`=======================================\`);
console.log(\`🎯 Measure, don't guess - Always profile first\`);
console.log(\`🎯 Fix the root cause, not symptoms\`);
console.log(\`🎯 Optimize for the common case\`);
console.log(\`🎯 Consider trade-offs (speed vs memory vs complexity)\`);
console.log(\`🎯 Test optimizations under realistic conditions\`);
console.log(\`🎯 Document performance requirements and SLAs\`);
console.log(\`🎯 Plan for graceful degradation under load\`);
`}
</InteractiveCodeBlock>

**Student**: "Wow! This optimization challenge really drives home how much impact the right techniques can have. I can see exactly where my current applications are slow and what to fix first."

**Teacher**: "Exactly! The key insight here is that performance optimization is detective work—you investigate, hypothesize, test, and measure. The 80% improvement we achieved isn't magic; it's systematic application of proven techniques. Most importantly, notice how we measured before and after each optimization. Without metrics, you're just guessing."

## Production Performance: Beyond Development

Understanding performance in production requires additional considerations that don't appear in development environments.

### Monitoring and Alerting

In production, you need continuous visibility into your application's performance:

```javascript
// Production monitoring checklist
const productionMonitoring = {
  metrics: {
    responseTime: 'P95 < 500ms',
    throughput: 'Requests per second',
    errorRate: '< 0.1%',
    availability: '99.9% uptime'
  },
  
  alerts: {
    slowResponses: 'P95 response time > 1000ms',
    highErrorRate: 'Error rate > 1%',
    memoryLeaks: 'Memory usage growing consistently',
    diskSpace: 'Disk usage > 80%'
  },
  
  tools: [
    'Application Performance Monitoring (APM)',
    'Log aggregation and analysis',
    'Infrastructure monitoring',
    'Real User Monitoring (RUM)'
  ]
};
```

### Performance Budgets

Set and enforce performance budgets to prevent regressions:

- **Response Time Budgets**: API endpoints under 200ms, page loads under 2 seconds
- **Resource Budgets**: Bundle sizes, memory usage, database query limits
- **Monitoring Budgets**: Alert thresholds that trigger when budgets are exceeded

### Graceful Degradation

Design systems that gracefully handle increased load:

```javascript
// Circuit breaker pattern for external services
class CircuitBreaker {
  constructor(failureThreshold = 5, timeoutWindow = 60000) {
    this.failureCount = 0;
    this.failureThreshold = failureThreshold;
    this.timeoutWindow = timeoutWindow;
    this.state = 'CLOSED'; // CLOSED, OPEN, HALF_OPEN
    this.nextAttempt = Date.now();
  }

  async execute(operation) {
    if (this.state === 'OPEN') {
      if (Date.now() < this.nextAttempt) {
        throw new Error('Circuit breaker is OPEN');
      }
      this.state = 'HALF_OPEN';
    }

    try {
      const result = await operation();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  onSuccess() {
    this.failureCount = 0;
    this.state = 'CLOSED';
  }

  onFailure() {
    this.failureCount++;
    if (this.failureCount >= this.failureThreshold) {
      this.state = 'OPEN';
      this.nextAttempt = Date.now() + this.timeoutWindow;
    }
  }
}
```

## What's Next: Advanced Performance Topics

This tutorial covered the fundamentals, but performance optimization is a continuous journey. Advanced topics to explore:

- **Microservice Performance**: Inter-service communication optimization
- **Database Performance**: Query optimization, indexing strategies, connection pooling
- **CDN and Edge Computing**: Geographic performance optimization
- **Progressive Web Apps**: Client-side performance optimization
- **Container Orchestration**: Kubernetes performance tuning
- **Observability**: Distributed tracing, metrics collection, log analysis

**Student**: "This has completely changed how I think about building applications. I used to focus only on features, but now I see performance as a feature itself."

**Teacher**: "That's the mindset shift that separates good developers from great ones! Performance isn't something you add at the end—it's woven into every decision from architecture to implementation. You now have the tools and knowledge to build applications that not only work correctly but perform beautifully under real-world conditions."

## Key Takeaways

1. **Measure First**: Never optimize without profiling and measuring actual bottlenecks
2. **Systematic Approach**: Use the 80/20 rule—fix the biggest problems first
3. **Multi-Layer Strategy**: Optimize at every layer—code, database, caching, infrastructure
4. **Monitor Continuously**: Performance optimization is an ongoing process, not a one-time task
5. **Plan for Scale**: Design systems that can grow gracefully with increased load

Performance optimization is both an art and a science. With the techniques you've learned here—profiling, memory management, caching, clustering, and systematic optimization—you're equipped to build Node.js applications that deliver exceptional user experiences at any scale.

Remember: Fast applications aren't just about speed—they're about respect for your users' time and creating experiences that delight rather than frustrate. Every millisecond matters.

Your journey from building functional applications to engineering high-performance systems starts with applying these concepts to your next project. Profile, optimize, measure, and repeat. The performance improvements will speak for themselves.
